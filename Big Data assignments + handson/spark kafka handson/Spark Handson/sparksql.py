# -*- coding: utf-8 -*-
"""SparkSQL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tyaQz3WDaVPH6oWILWDS1IkDfypuQ1Cv
"""

!sudo apt update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark
!pip install pyspark
!pip install py4j

import os
import sys
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"


import findspark
findspark.init()
findspark.find()

import pyspark

from pyspark.sql import DataFrame, SparkSession
from typing import List
import pyspark.sql.types as T
import pyspark.sql.functions as F

spark= SparkSession \
       .builder \
       .appName("SparkSQL Example") \
       .getOrCreate()

spark

import requests
path="https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv"
req = requests.get(path)
url_content = req.content
csv_file_name = 'employees.csv'
csv_file = open(csv_file_name, 'wb')
csv_file.write(url_content)
csv_file.close()
df = spark.read.csv(csv_file_name, header=True, inferSchema=True)

# URL for the raw content of your Gist
path = "https://gist.githubusercontent.com/suprkar/1e43b1d7971e03080cedd9ee25294401/raw/departments.csv"

# Fetch the content
req = requests.get(path)
url_content = req.content

# Write content to a local file
csv_file_name = 'departments.csv'
with open(csv_file_name, 'wb') as csv_file:
    csv_file.write(url_content)

# Read the CSV file using Spark
departments_df = spark.read.csv(csv_file_name, header=True, inferSchema=True)

# Show the first few rows to verify
departments_df.show(5)

"""# PySpark DataFrames"""

df.printSchema()

df.show()

df.select(F.to_date(df.HIRE_DATE).alias('date'))

df.filter(df.FIRST_NAME == "John").orderBy(F.desc("HIRE_DATE")).show()

df.groupBy("DEPARTMENT_ID").sum("SALARY").orderBy(F.desc("sum(SALARY)")).show(truncate=False)

"""# SPARK SQL"""

df.createOrReplaceTempView("emp_data")

df2=spark.sql("SELECT * from emp_data")
df2.printSchema()

groupDF=spark.sql("select DEPARTMENT_ID, count(*) from emp_data group by DEPARTMENT_ID")
groupDF.show()

df.show(5)

df.count()

df.select("FIRST_NAME","SALARY").show(5)

df.select("JOB_ID").distinct().show()

from pyspark.sql import functions as F
test=df.groupBy("DEPARTMENT_ID").agg(F.sum("SALARY"))

test.toPandas()

"""# Creating a test spark df"""

data=[
    ("John","Smith","20-10-2000",'M',12000),
    ("James","Johnson","20-10-1990",'M',42000),
    ("Mark","Taylow","10-10-1995",'M',32000),
    ("John","Corn","20-10-2000",'M',12000),
    ("John","Smith","20-10-2000",'M',15000)
]
columns=["Fname","Lname","dob","gender","salary"]
df=spark.createDataFrame(data=data, schema=columns)

df.show()

df.dropDuplicates(["Fname","Lname"]).show()

df.dropDuplicates(["Fname","Lname"]).groupBy("Fname").count().sort("count").show()

"""# 1. Creating new dataframe for employees dataset

"""

import requests
from pyspark.sql.functions import col

# Fetch the employee data
path = "https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv"
req = requests.get(path)
url_content = req.content
csv_file_name = 'employees.csv'
with open(csv_file_name, 'wb') as csv_file:
    csv_file.write(url_content)

# Create the employees DataFrame
employees_df = spark.read.csv(csv_file_name, header=True, inferSchema=True)

# Convert HIRE_DATE to date type
employees_df = employees_df.withColumn("HIRE_DATE", F.to_date(col("HIRE_DATE"), "dd-MMM-yy"))

print("Employees DataFrame schema:")
employees_df.printSchema()

print("\nEmployees DataFrame sample data:")
employees_df.show(5)

"""# 1.Inner Join"""

inner_join = employees_df.join(departments_df, "DEPARTMENT_ID", "inner")
print("Inner Join Result:")
inner_join.select("EMPLOYEE_ID", "FIRST_NAME", "LAST_NAME", "DEPARTMENT_ID", "DEPARTMENT_NAME", "LOCATION").show(10)

"""# Right Join"""

right_join = employees_df.join(departments_df, "DEPARTMENT_ID", "right")
print("\nRight Join Result:")
right_join.select("EMPLOYEE_ID", "FIRST_NAME", "LAST_NAME", "DEPARTMENT_ID", "DEPARTMENT_NAME", "LOCATION").show(10)

"""# Full Outer Join"""

full_join = employees_df.join(departments_df, "DEPARTMENT_ID", "full")
print("\nFull Outer Join Result:")
full_join.select("EMPLOYEE_ID", "FIRST_NAME", "LAST_NAME", "DEPARTMENT_ID", "DEPARTMENT_NAME", "LOCATION").show(10)

"""# Count employees in each department"""

dept_employee_count = inner_join.groupBy("DEPARTMENT_ID", "DEPARTMENT_NAME").count().orderBy("DEPARTMENT_ID")
print("\nEmployee count by department:")
dept_employee_count.show()

"""# Find departments with no employees"""

depts_no_employees = departments_df.join(employees_df, "DEPARTMENT_ID", "left_anti")
print("\nDepartments with no employees:")
depts_no_employees.show()

"""# Find employees with no department"""

employees_no_dept = employees_df.join(departments_df, "DEPARTMENT_ID", "left_anti")
print("\nEmployees with no department:")
employees_no_dept.show()